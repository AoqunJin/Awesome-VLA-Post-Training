# :bookmark: Awesome-Embodied-AI-Tuning

**Awesome-Embodied-AI-Tuning** is a continuously updated collection of cutting-edge resources focused on **tuning embodied AI systems**. As embodied AI experiences rapid growth, this repository serves as a centralized hub for research updates, practical codes, and implementation insights. Our goal is to enhance the ability of AI agents to perceive, reason, and act within physical environments. Key focus areas include:

* :earth_asia: **Enhancing environmental perception**
* :brain: **Improving embodiment awareness**
* :memo: **Deepening task comprehension and generalization**
* :wrench: **Integrating and tuning multiple components**

We welcome contributions from researchers and practitioners passionate about advancing generative embodied intelligence. Join us in building a structured, high-quality resource for the community!

---

## :star: Notable Works

A curated selection of influential papers, benchmarks, and projects that have significantly contributed to the field of generative embodied AI. These works provide foundational insights and state-of-the-art methods that inform current research directions.

* **[2022-12]** RT-1: Robotics Transformer for real-world control at scale. ([Paper](https://arxiv.org/pdf/2212.06817), [Website](https://robotics-transformer1.github.io), [Code](https://github.com/google-research/robotics_transformer))

* **[2023-07]** RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. ([Paper](https://arxiv.org/abs/2307.15818), [Website](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action))

* **[2023-10]** Octopus: Embodied Vision-Language Programmer from Environmental Feedback. ([Paper](https://arxiv.org/abs/2310.08588), [Website](https://choiszt.github.io/Octopus), [Code](https://github.com/dongyh20/Octopus))

* **[2023-11]** Vision-Language Foundation Models as Effective Robot Imitators. ([Paper](https://arxiv.org/abs/2311.01378), [Website](https://roboflamingo.github.io), [Code](https://github.com/RoboFlamingo/RoboFlamingo))

* **[2023-12]** Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. ([Paper](https://arxiv.org/abs/2312.13139), [Website](https://gr1-manipulation.github.io), [Code](https://github.com/GR1-Manipulation/GR-1))

* **[2024-03]** 3D-VLA: A 3D Vision-Language-Action Generative World Model. ([Paper](https://arxiv.org/abs/2403.09631), [Website](https://vis-www.cs.umass.edu/3dvla), [Code](https://github.com/UMass-Embodied-AGI/3D-VLA))

* **[2024-05]** Octo: An Open-Source Generalist Robot Policy. ([Paper](https://arxiv.org/abs/2405.12213), [Website](https://octo-models.github.io), [Code](https://github.com/octo-models/octo))

* **[2024-06]** OpenVLA: An Open-Source Vision-Language-Action Model. ([Paper](https://arxiv.org/abs/2406.09246), [Website](https://openvla.github.io), [Code](https://github.com/openvla/openvla))

* **[2024-06]** RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation. ([Paper](https://arxiv.org/abs/2406.04339), [Website](https://sites.google.com/view/robomamba-web), [Code](https://github.com/lmzpai/roboMamba))

* **[2024-10]** RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation. ([Paper](https://arxiv.org/abs/2410.07864), [Website](https://rdt-robotics.github.io/rdt-robotics), [Code](https://github.com/thu-ml/RoboticsDiffusionTransformer))

* **[2024-10]** π0: A Vision-Language-Action Flow Model for General Robot Control. ([Paper](https://arxiv.org/abs/2410.24164), [Website](https://www.physicalintelligence.company/blog/pi0), [Code](https://github.com/Physical-Intelligence/openpi))

* **[2024-11]** CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation. ([Paper](https://arxiv.org/abs/2411.19650), [Website](https://cogact.github.io), [Code](https://github.com/microsoft/CogACT))

* **[2025-04]** π05a Vision-Language-Action Model with Open-World Generalization. ([Paper](https://arxiv.org/abs/2504.16054), [Website](https://www.physicalintelligence.company/blog/pi05))

* **[2025-05]** UniVLA: Learning to Act Anywhere with Task-centric Latent Actions. ([Paper](https://arxiv.org/abs/2505.06111), [Code](https://github.com/OpenDriveLab/UniVLA))

---

## :earth_asia: Enhancing Environmental Perception

This section explores methods that improve an agent’s ability to perceive and interpret its environment. It includes **affordance-guided learning**, which enables agents to understand actionable properties of objects; **enhanced encoders** tailored for manipulation tasks, allowing more precise feature extraction; and **improved representation learning**, which helps models build richer and more structured environmental understanding for downstream tasks.

### Affordance-Guided Learning

### Enhanced Encoder for Manipulation

### Enhanced Representation for Manipulation

---

## :brain: Improving Embodiment Awareness

Here we focus on helping agents better understand their own physical structure and capabilities. Topics include **forward and inverse kinematics learning**, which allow agents to model the relationship between joint movements and spatial positions, and **action head design**, aimed at optimizing how high-level decisions are translated into low-level motor commands.

### Forward kinematics learning

### Inverse kinematics learning

### Action Head Designing

---

## :memo: Deepening Task Comprehension

This section covers methods that enable agents to better understand and generalize across tasks. Key areas include **human–robot interaction**, where agents learn to interpret and respond to human inputs effectively, and **hierarchical task manipulation**, which enables multi-step reasoning and planning by decomposing complex tasks into structured subtasks.

### Human–Robot-Interaction

### Hierarchical Task Manipulation

---

## :wrench: Multiple Component Integration

Integrating various subsystems is essential for building robust embodied agents. This section includes **reinforcement learning frameworks** for continuous control and decision-making, **visual interaction prediction** for anticipating future outcomes based on perception, and strategies for **data-efficient fine-tuning** to reduce the cost of adapting models to new environments or tasks.

### Reinforcement Learning

### Visual Interaction Prediction

### Data-effective Fine-tuning

---

## :black_nib: Contributing

We welcome contributions from the community! Whether it's adding new papers, sharing code, or improving documentation, your input helps make this a valuable resource for everyone!

---

## :pushpin: BibTeX

To cite this repository in your research, please use the following BibTeX entry:

```bibtex
@misc{awesome_embodied_ai_tuning,
  title       = {Awesome-Embodied-AI-Tuning},
  author      = {Contributors},
  year        = {2025},
  howpublished= {\url{https://github.com/AoqunJin/Awesome-Embodied-AI-Tuning}},
  note        = {A curated list of resources for tuning and enhancing embodied AI systems}
}
```
